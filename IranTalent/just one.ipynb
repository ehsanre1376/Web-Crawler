{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\EHSANRE\\\\OneDrive\\\\Web Crawler\\\\Page Number.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m site \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/companies\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(site\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mEHSANRE\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mOneDrive\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mWeb Crawler\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mPage Number.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m my_file:\n\u001b[0;32m     13\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(my_file\u001b[38;5;241m.\u001b[39mread())\n",
      "File \u001b[1;32mc:\\Users\\EHSANRE\\OneDrive\\Web_Crawler\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\EHSANRE\\\\OneDrive\\\\Web Crawler\\\\Page Number.txt'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "url = r\"https://www.irantalent.com\"\n",
    "site = requests.get(url + \"/companies\")\n",
    "soup = BeautifulSoup(site.text, \"html.parser\")\n",
    "with open(r\"C:\\Users\\EHSANRE\\OneDrive\\Web Crawler\\Page Number.txt\") as my_file:\n",
    "    n = int(my_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_page():\n",
    "    driver = webdriver.Edge()\n",
    "    driver.get(url + \"/companies\")\n",
    "    show_more_button = driver.find_element(\n",
    "        By.CSS_SELECTOR,\n",
    "        \"#companies-brands-list > div > div.d-flex.align-items-center.pagination-wrapper > button.page.action-next-page.flipArrowKeys\",\n",
    "    )\n",
    "\n",
    "    show_more_button.click()\n",
    "def read_web(Surl):\n",
    "    Ssite = requests.get(Surl)\n",
    "    Ssoup = BeautifulSoup(Ssite.text, \"html.parser\")\n",
    "    # Site---------------\n",
    "    courses = Ssoup.select(\n",
    "        \"body > app-root > div > div > div > div > company-branding > div > div.row.d-flex.flex-wrap > div.col-md-9.col-xs-12 > brand-overview > div > form > div.card-section.company-information.padding-top-16.ng-untouched.ng-pristine.ng-valid > div > div > div > div > div.col-md-7.col-xs-6 > a\"\n",
    "    )\n",
    "    siteLink = courses[0].get(\"href\")\n",
    "    # CEO-------------------\n",
    "    courses = Ssoup.select(\n",
    "    \"body > app-root > div > div > div > div > company-branding > div > div.row.d-flex.flex-wrap > div.col-md-9.col-xs-12 > brand-overview > div > form > div.card-section.company-information.padding-top-16.ng-untouched.ng-pristine.ng-valid > div > div > div > div > div.col-md-7.col-xs-6 > p\"\n",
    "    )\n",
    "\n",
    "    ceo=courses[0].text.strip()\n",
    "    # Date--------------------------------\n",
    "    courses = Ssoup.select(\n",
    "    \"body > app-root > div > div > div:nth-child(2) > div > company-branding > div > div.row.d-flex.flex-wrap > div.col-md-9.col-xs-12 > brand-overview > div > form > div.card-section.company-information.padding-top-16.ng-untouched.ng-pristine.ng-valid > div > div > div:nth-child(1) > div:nth-child(3) > div.col-md-7.col-xs-6 > p\"\n",
    "    )\n",
    "\n",
    "    date=courses[0].text.strip()\n",
    "    # address---------------\n",
    "    courses = Ssoup.select(\n",
    "    \"body > app-root > div > div > div:nth-child(2) > div > company-branding > div > div.row.d-flex.flex-wrap > div.col-md-9.col-xs-12 > brand-overview > div > form > div.card-section.company-information.padding-top-16.ng-untouched.ng-pristine.ng-valid > div > div > div:nth-child(2) > div:nth-child(2) > div.col-md-7.col-xs-6 > p\"\n",
    "    )\n",
    "\n",
    "    address=courses[0].text.strip()\n",
    "    # Location----------------\n",
    "    courses = Ssoup.select(\n",
    "    \"body > app-root > div > div > div:nth-child(2) > div > company-branding > div > div.row.d-flex.flex-wrap > div.col-md-9.col-xs-12 > brand-overview > div > form > div.card-section.company-information.padding-top-16.ng-untouched.ng-pristine.ng-valid > div > div > div:nth-child(2) > div:nth-child(1) > div.col-md-7.col-xs-6 > p\"\n",
    ")\n",
    "\n",
    "    Location=courses[0].text.strip().replace(\",\", \"\")\n",
    "    return siteLink, ceo, date, address, Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              title                                              Links  \\\n",
      "0  خدمات انفورماتیک  https://www.irantalent.com/company/informatics...   \n",
      "\n",
      "                                   industrial           personels  \\\n",
      "0  فناوری اطلاعات، خدمات اینترنتی و نرم افزار  500 کارمند به بالا   \n",
      "\n",
      "               siteLink ceo  date                 address Location  \n",
      "0  http://www.isc.co.ir      1993  تهران، خیابان میرداماد    تهران  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'with open(\"Page Number.txt\", \"w\") as my_file:\\n    my_file.write(str(i))\\nchange_page()'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop for every page\n",
    "# df = pd.read_excel(r\"C:\\Users\\EHSANRE\\OneDrive\\Web Crawler\\IranTalent.xlsx\")\n",
    "titles\t=Links\t=industrial=\tpersonels\t=siteLink\t=ceo=\tdate\t=address\t=Location=[]\n",
    "\n",
    "\n",
    "url = r\"https://www.irantalent.com\"\n",
    "site = requests.get(url + \"/companies\")\n",
    "soup = BeautifulSoup(site.text, \"html.parser\")\n",
    "# titles---------------------------------------\n",
    "titles = soup.select(\"div.card-title.d-flex.flex-column.padding-all-16 > p[title]\")\n",
    "\n",
    "# Extract the title attribute from each p element\n",
    "titles = [elem[\"title\"].strip() for elem in titles][0]\n",
    "# Links----------------------------------------\n",
    "courses = soup.select(\n",
    "\"body > app-root > div > div > div > div > it-company-list > div > div > div.container.w-100.padding-x-12.mobile-padding-x-0.main-wrapper > div.padding-y-24.mobile-padding-y-12 > div > div > div > a \"\n",
    ")\n",
    "overviewLink = [url + link.get(\"href\") for link in courses][0]\n",
    "# industrial--------------------------------\n",
    "courses = soup.select(\n",
    "\"body > app-root > div > div > div > div > it-company-list > div > div > div.container.w-100.padding-x-12.mobile-padding-x-0.main-wrapper > div.padding-y-24.mobile-padding-y-12 > div > div > div > a > div.padding-x-16.d-flex.text-decoration-none > p.font-weight-500.font-12.color-gray.margin-bottom-16.text-ellipsis.max-w-50.margin-right-8\"\n",
    ")\n",
    "\n",
    "industrial = [industrial.text.strip().replace(\"\\u200c\", \" \") for industrial in courses][0]\n",
    "# personels------------------------------\n",
    "courses = soup.select(\n",
    "\"body > app-root > div > div > div > div > it-company-list > div > div > div.container.w-100.padding-x-12.mobile-padding-x-0.main-wrapper > div.padding-y-24.mobile-padding-y-12 > div > div > div > a > div.padding-x-16.d-flex.text-decoration-none > p.d-flex.font-weight-500.font-12.color-gray.margin-bottom-16.max-w-50\"\n",
    ")\n",
    "\n",
    "\n",
    "personels = [personels.text.strip() for personels in courses][0]\n",
    "\n",
    "siteLink, ceo, date, address, Location = read_web(overviewLink)\n",
    "data = pd.DataFrame(\n",
    "    {\n",
    "        \"title\": titles,\n",
    "        \"Links\": overviewLink,\n",
    "        \"industrial\": industrial,\n",
    "        \"personels\": personels,\n",
    "        \"siteLink\": siteLink,\n",
    "        \"ceo\": ceo,\n",
    "        \"date\": date,\n",
    "        \"address\": address,\n",
    "        \"Location\": Location,\n",
    "    },\n",
    "    index=[0],\n",
    ")\n",
    "\n",
    "print(data)\n",
    "data.to_excel(\"output.xlsx\")\n",
    "# df.append(data)\n",
    "# appended_data.to_excel(r'C:\\Users\\EHSANRE\\OneDrive\\Web Crawler\\IranTalent.xlsx', index=False)\n",
    "\"\"\"with open(\"Page Number.txt\", \"w\") as my_file:\n",
    "    my_file.write(str(i))\n",
    "change_page()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "async def fetch(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        return await response.text()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    url = r\"https://www.irantalent.com\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        html = await fetch(session, url + \"/companies\")\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        # Your scraping code here...\n",
    "\n",
    "\n",
    "# Run the main function\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "url = r\"https://www.irantalent.com\"\n",
    "site = requests.get(url + \"/companies\")\n",
    "soup = BeautifulSoup(site.text, \"html.parser\")\n",
    "\n",
    "def read_web(Surl):\n",
    "    Ssite = requests.get(Surl)\n",
    "    Ssoup = BeautifulSoup(Ssite.text, \"html.parser\")\n",
    "    # Site---------------\n",
    "    courses = Ssoup.select(\n",
    "        \"body > app-root > div > div > div > div > company-branding > div > div.row.d-flex.flex-wrap > div.col-md-9.col-xs-12 > brand-overview > div > form > div.card-section.company-information.padding-top-16.ng-untouched.ng-pristine.ng-valid > div > div > div > div > div.col-md-7.col-xs-6 > a\"\n",
    "    )\n",
    "    siteLink = courses[0].get(\"href\")\n",
    "    return siteLink\n",
    "\n",
    "\n",
    "address = siteLink = []\n",
    "\n",
    "\n",
    "url = r\"https://www.irantalent.com\"\n",
    "site = requests.get(url + \"/companies\")\n",
    "soup = BeautifulSoup(site.text, \"html.parser\")\n",
    "\n",
    "# Links----------------------------------------\n",
    "courses = soup.select(\n",
    "    \"body > app-root > div > div > div > div > it-company-list > div > div > div.container.w-100.padding-x-12.mobile-padding-x-0.main-wrapper > div.padding-y-24.mobile-padding-y-12 > div > div > div > a \"\n",
    ")\n",
    "overviewLink = [url + link.get(\"href\") for link in courses][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "address= [read_web(wLink) for wlink in overviewLink]\n",
    "data = pd.DataFrame(\n",
    "    {\n",
    "        \n",
    "        \"Links\": overviewLink,\n",
    "        \n",
    "        \"address\": address,\n",
    "        \n",
    "    },\n",
    "    index=[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "\n",
    "url = r\"https://www.irantalent.com\"\n",
    "\n",
    "\n",
    "async def read_web(session, Surl):\n",
    "    async with session.get(Surl) as Ssite:\n",
    "        text = await Ssite.text()\n",
    "        Ssoup = BeautifulSoup(text, \"html.parser\")\n",
    "        courses = Ssoup.select(\n",
    "            \"body > app-root > div > div > div > div > company-branding > div > div.row.d-flex.flex-wrap > div.col-md-9.col-xs-12 > brand-overview > div > form > div.card-section.company-information.padding-top-16.ng-untouched.ng-pristine.ng-valid > div > div > div > div > div.col-md-7.col-xs-6 > a\"\n",
    "        )\n",
    "        siteLink = courses[0].get(\"href\")\n",
    "        return siteLink\n",
    "\n",
    "\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        site = await session.get(url + \"/companies\")\n",
    "        text = await site.text()\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "\n",
    "        courses = soup.select(\n",
    "            \"body > app-root > div > div > div > div > it-company-list > div > div > div.container.w-100.padding-x-12.mobile-padding-x-0.main-wrapper > div.padding-y-24.mobile-padding-y-12 > div > div > div > a \"\n",
    "        )\n",
    "        overviewLink = [url + link.get(\"href\") for link in courses][0]\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            futures = [\n",
    "                loop.run_in_executor(executor, read_web, session, wLink)\n",
    "                for wLink in overviewLink\n",
    "            ]\n",
    "            address = await asyncio.gather(*futures)\n",
    "\n",
    "        data = pd.DataFrame(\n",
    "            {\n",
    "                \"Links\": overviewLink,\n",
    "                \"address\": address,\n",
    "            },\n",
    "            index=[0],\n",
    "        )\n",
    "\n",
    "\n",
    "# Run the main function\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, '', '', '', '', 'تولید کننده فرش ایتالیایی', '-', '-')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ReadOwerview(url):\n",
    "    try:\n",
    "        site = requests.get(url)\n",
    "        soup = BeautifulSoup(site.text, \"html.parser\")\n",
    "\n",
    "    except Exception:\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            site = requests.get(url)\n",
    "            soup = BeautifulSoup(site.text, \"html.parser\")\n",
    "\n",
    "        except Exception:\n",
    "            time.sleep(5)\n",
    "            site = requests.get(url)\n",
    "            soup = BeautifulSoup(site.text, \"html.parser\")\n",
    "\n",
    "    # Site---------------------------------------------\n",
    "    courses = soup.select(\n",
    "        \"body > app-root > div > div > div:nth-child(2) > div > company-branding > div > div.row.d-flex.flex-wrap > div.col-md-9.col-xs-12 > brand-overview > div > form > div > div > div > div:nth-child(1) > div:nth-child(1) > div.col-md-7.col-xs-6\"\n",
    "    )\n",
    "    try:\n",
    "        siteLink = courses[0].get(\"href\")\n",
    "    except Exception:\n",
    "        siteLink = \"#\"\n",
    "\n",
    "    # CEO-------------------------------------------------------\n",
    "    courses = soup.select(\n",
    "        \"body > app-root > div > div > div:nth-child(2) > div > company-branding > div > div.row.d-flex.flex-wrap > div.col-md-9.col-xs-12 > brand-overview > div > form > div > div > div > div:nth-child(1) > div:nth-child(2) > div.col-md-7.col-xs-6\"\n",
    "    )\n",
    "    try:\n",
    "        ceo = courses[0].text.strip()\n",
    "    except Exception:\n",
    "        ceo = None\n",
    "\n",
    "    # CreatDate------------------------------------------------------------\n",
    "    courses = soup.select(\n",
    "        \"body > app-root > div > div > div:nth-child(2) > div > company-branding > div > div.row.d-flex.flex-wrap > div.col-md-9.col-xs-12 > brand-overview > div > form > div > div > div > div:nth-child(1) > div:nth-child(3) > div.col-md-7.col-xs-6\"\n",
    "    )\n",
    "    try:\n",
    "        date = courses[0].text.strip()\n",
    "    except Exception:\n",
    "        date = None\n",
    "\n",
    "    # address-------------------------------------\n",
    "    courses = soup.select(\n",
    "        \"body > app-root > div > div > div:nth-child(2) > div > company-branding > div > div.row.d-flex.flex-wrap > div.col-md-9.col-xs-12 > brand-overview > div > form > div > div > div > div:nth-child(2) > div:nth-child(2) > div.col-md-7.col-xs-6\"\n",
    "    )\n",
    "    try:\n",
    "        address = courses[0].text.strip()\n",
    "    except Exception:\n",
    "        address = None\n",
    "\n",
    "    # Location------------------------------------------------\n",
    "    courses = soup.select(\n",
    "        \"body > app-root > div > div > div:nth-child(2) > div > company-branding > div > div.row.d-flex.flex-wrap > div.col-md-9.col-xs-12 > brand-overview > div > form > div > div > div > div:nth-child(2) > div:nth-child(1) > div.col-md-7.col-xs-6\"\n",
    "    )\n",
    "    try:\n",
    "        Location = courses[0].text.strip().replace(\",\", \"\")\n",
    "    except Exception:\n",
    "        Location = None\n",
    "\n",
    "    # Titles-----------------------------------------------\n",
    "    courses = soup.select(\n",
    "        \"body > app-root > div > div > div:nth-child(2) > div > company-branding > div > div.row.d-flex.flex-wrap > div.col-md-9.col-xs-12 > brand-overview > div > form > div > h1\"\n",
    "    )\n",
    "    Titles = courses[0].text.strip().replace(\"درباره \", \"\")\n",
    "    try:\n",
    "        Titles = courses[0].text.strip().replace(\"درباره \", \"\")\n",
    "    except Exception:\n",
    "        Titles = None\n",
    "\n",
    "    # Industrial-----------------------------------------------\n",
    "    courses = soup.select(\n",
    "        \"body > app-root > div > div > div:nth-child(2) > div > company-branding > div > div.row.d-flex.flex-wrap > div.col-md-9.col-xs-12 > brand-overview > div > form > div > div > div > div:nth-child(1) > div:nth-child(4) > div.col-md-7.col-xs-6\"\n",
    "    )\n",
    "    try:\n",
    "        Industrial = courses[0].text.strip()\n",
    "    except Exception:\n",
    "        Industrial = None\n",
    "\n",
    "    # personnel------------------------------------------------------\n",
    "    courses = soup.select(\n",
    "        \"body > app-root > div > div > div:nth-child(2) > div > company-branding > div > div.row.d-flex.flex-wrap > div.col-md-9.col-xs-12 > brand-overview > div > form > div > div > div > div:nth-child(1) > div:nth-child(5) > div.col-md-7.col-xs-6\"\n",
    "    )\n",
    "    try:\n",
    "        personnel = courses[0].text.strip()\n",
    "    except Exception:\n",
    "        personnel = None\n",
    "    # ------------------------------------------------------\n",
    "    return siteLink, ceo, date, address, Location, Titles, Industrial, personnel\n",
    "ReadOwerview(\n",
    "    \"https://www.irantalent.com/company/luigi-sartori/6d86c7d0-84a6-4c02-8ef8-7f8d6242e689/overview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New\n",
    "\n",
    "def ReadOwerview(url):\n",
    "    try:\n",
    "        site = requests.get(url)\n",
    "        soup = BeautifulSoup(site.text, \"html.parser\")\n",
    "\n",
    "    except Exception:\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            site = requests.get(url)\n",
    "            soup = BeautifulSoup(site.text, \"html.parser\")\n",
    "\n",
    "        except Exception:\n",
    "            time.sleep(5)\n",
    "            site = requests.get(url)\n",
    "            soup = BeautifulSoup(site.text, \"html.parser\")\n",
    "\n",
    "    # Site---------------------------------------------\n",
    "    courses = soup.select(\n",
    "        \"body > app-root > div > div > div > div > company-branding > div > div.row.d-flex.flex-wrap > div.col-md-9.col-xs-12 > brand-overview > div > form > div.card-section.company-information.padding-top-16.ng-untouched.ng-pristine.ng-valid > div > div > div > div > div.col-md-7.col-xs-6 > a\"\n",
    "    )\n",
    "    try:\n",
    "        siteLink = courses[0].get(\"href\")\n",
    "    except Exception:\n",
    "        siteLink = \"#\"\n",
    "\n",
    "    # CEO-------------------------------------------------------\n",
    "    courses = soup.select(\n",
    "        \"body > app-root > div > div > div > div > company-branding > div > div.row.d-flex.flex-wrap > div.col-md-9.col-xs-12 > brand-overview > div > form > div.card-section.company-information.padding-top-16.ng-untouched.ng-pristine.ng-valid > div > div > div > div > div.col-md-7.col-xs-6 > p\"\n",
    "    )\n",
    "    try:\n",
    "        ceo = courses[0].text.strip()\n",
    "    except Exception:\n",
    "        ceo = None\n",
    "\n",
    "    # CreatDate------------------------------------------------------------\n",
    "    courses = soup.select(\n",
    "        \"body > app-root > div > div > div:nth-child(2) > div > company-branding > div > div.row.d-flex.flex-wrap > div.col-md-9.col-xs-12 > brand-overview > div > form > div.card-section.company-information.padding-top-16.ng-untouched.ng-pristine.ng-valid > div > div > div:nth-child(1) > div:nth-child(3) > div.col-md-7.col-xs-6 > p\"\n",
    "    )\n",
    "    try:\n",
    "        date = courses[0].text.strip()\n",
    "    except Exception:\n",
    "        date = None\n",
    "\n",
    "    # address-------------------------------------\n",
    "    courses = soup.select(\n",
    "        \"body > app-root > div > div > div:nth-child(2) > div > company-branding > div > div.row.d-flex.flex-wrap > div.col-md-9.col-xs-12 > brand-overview > div > form > div.card-section.company-information.padding-top-16.ng-untouched.ng-pristine.ng-valid > div > div > div:nth-child(2) > div:nth-child(2) > div.col-md-7.col-xs-6 > p\"\n",
    "    )\n",
    "    try:\n",
    "        address = courses[0].text.strip()\n",
    "    except Exception:\n",
    "        address = None\n",
    "\n",
    "    # Location------------------------------------------------\n",
    "    courses = soup.select(\n",
    "        \"body > app-root > div > div > div:nth-child(2) > div > company-branding > div > div.row.d-flex.flex-wrap > div.col-md-9.col-xs-12 > brand-overview > div > form > div.card-section.company-information.padding-top-16.ng-untouched.ng-pristine.ng-valid > div > div > div:nth-child(2) > div:nth-child(1) > div.col-md-7.col-xs-6 > p\"\n",
    "    )\n",
    "    try:\n",
    "        Location = courses[0].text.strip().replace(\",\", \"\")\n",
    "    except Exception:\n",
    "        Location = None\n",
    "\n",
    "    # Titles-----------------------------------------------\n",
    "    courses = soup.select(\n",
    "        \"body > app-root > div > div > div:nth-child(2) > div > company-branding > div > div.row.d-flex.flex-wrap > div.col-md-9.col-xs-12 > brand-overview > div > form > div > h1\"\n",
    "    )\n",
    "    Titles = courses[0].text.strip().replace(\"درباره \", \"\")\n",
    "    try:\n",
    "        Titles = courses[0].text.strip().replace(\"درباره \", \"\")\n",
    "    except Exception:\n",
    "        Titles = None\n",
    "\n",
    "    # Industrial-----------------------------------------------\n",
    "    courses = soup.select(\n",
    "        \"body > app-root > div > div > div:nth-child(2) > div > company-branding > div > div.row.d-flex.flex-wrap > div.col-md-9.col-xs-12 > brand-overview > div > form > div.card-section.company-information.padding-top-16.ng-untouched.ng-pristine.ng-valid > div > div > div:nth-child(1) > div:nth-child(4) > div.col-md-7.col-xs-6 > p\"\n",
    "    )\n",
    "    try:\n",
    "        Industrial = courses[0].text.strip()\n",
    "    except Exception:\n",
    "        Industrial = None\n",
    "\n",
    "    # personnel------------------------------------------------------\n",
    "    courses = soup.select(\n",
    "        \"body > app-root > div > div > div:nth-child(2) > div > company-branding > div > div.row.d-flex.flex-wrap > div.col-md-9.col-xs-12 > brand-overview > div > form > div.card-section.company-information.padding-top-16.ng-untouched.ng-pristine.ng-valid > div > div > div:nth-child(1) > div:nth-child(5) > div.col-md-7.col-xs-6 > p\"\n",
    "    )\n",
    "    try:\n",
    "        personnel = courses[0].text.strip()\n",
    "    except Exception:\n",
    "        personnel = None\n",
    "    # ------------------------------------------------------\n",
    "    return siteLink, ceo, date, address, Location, Titles, Industrial, personnel\n",
    "\n",
    "\n",
    "ReadOwerview(\n",
    "    \"https://www.irantalent.com/company/luigi-sartori/6d86c7d0-84a6-4c02-8ef8-7f8d6242e689/overview\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
